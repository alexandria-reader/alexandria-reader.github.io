[{"content":"Current “learning with text” software applications fall under two broad groups.\nOne group includes long-standing open source projects that are well received but sparsely maintained, or newer free applications that started with a bang, but were quickly forgotten and subsequently neglected.\nThe second group of applications are commercial solutions that address the goal of language learning with text, but suffer from feature bloat, lack of user friendliness, or mobile performance issues.\nAlexandria would like to become the go-to solution for language learning enthusiasts. Through keeping the product free and open source, we aspire to continue developing product features that are crucial to language learners, while maintaining a joyful user experience based on the latest technological offerings.\n    Existing Free/Open Source Software Alexandria Commercial Products     Interactive front-end user experience ❌ ✅ ✅   Open Source ✅ ✅ ❌   Ongoing Development ❌ ✅ ✅   Mobile First ❌ ✅ ✅   Responsiveness to Community Input ❌ ✅ ✅   Phrase Selection ❌ ✅ ❓    Our main objectives at the initial stage are to: create a modern, fast, open source webapp with a mobile first interface. This will allow users to quickly and easily start learning without struggles with the interface, or excessive wait times for content to load.\n","description":"","tags":null,"title":"Competitor Analysis","uri":"/competitors/"},{"content":"Technical Decisions Every project starts with a number of technical decisions, Alexandria is no different. From programming language, database selection, to front-end library, a number of concious choices were made to match the scope and demands of the application.\n","description":"","tags":null,"title":"Technical Decisions","uri":"/decisions/"},{"content":"Since its inception, Typescript has been touted as a supercharged version of Javascript that has the ability to spot common errors through a typed system, making the application more robust throughout its development.\nPrior to developing Alexandria, we looked into Typescript as a feasible substitute to Javascript.\nStatic typing Javascript is dynamically typed. This means it is not aware of variable type before instantiating it at run time, where it may already be too late. TypeScript, on the other hand, is statically typed. Once an object’s type is declared, it can no longer be changed and will only take certain values. Adding type support to Javascript allows Typescript to catch type errors during compilation. Bugs caused by false assumptions of variable type can potentially be completely eradicated.\nSuppose we want to create an object called UserWord that represents how a user is saved to the database by adding a translation and status.\ntype UserWord = { id?: number, word: string, status?: Status, translations: Array\u003cTranslation\u003e, languageId?: string }; Type UserWord has 5 properties with predefined types. If languageId was added as number, Typescript would immediately warn us about the wrong type.\nAn even better example is the translations property, which takes an array of Translation type objects. If an attempt was made to add an object not of type Translation, we would again be warned by the compiler.\nIf an attempt was made to add a property not yet declared on the type, for example, language, the compiler would warn us that the property language does not exist on type UserWord.\nCompiler warnings could alert us to the existence of bugs ranging from type-related bugs to misspelled variables, before the code was even run. Over the course of development, this could save significant time and frustration, as far fewer errors would make it into the codebase.\nCode readability TypeScript has the potential to increases code readiability.\nLet’s take a function that takes an object as a parameter. In JavaScript, a developer would have to be familiar with the object and its properties, or possibly log the object to the console to see what properties exist on the object. With TypeScript in VSCode, a developer can simply mouse over the type declaration to see a list of the properties that object will have, as well as the each property type.\nEasy refactoring Validating connections between different parts of a project is time consuming and error-prone, especially when one needs to refactor. By providing instant feedback during development, TypeScript makes refactoring a breeze. For example, if a function name is changed in one file but not in another, the TypeScript compiler will immediately alert developer to the issue, showing the exact file and line of code where the problem resides. This allows developers feel more confident in the code, and saves considerable time in validating no part of the project is broken during refactoring.\nFuture advantages As the codebase grows, having knowledge over the properties of an object increases code stability, and makes it easier to onboard future contributors to Alexandria.\nDisadvantages of TypeScript Although TypeScript is a productivity booster when used correctly, there are disadvantages.\nThe typing system can be difficult to use properly, resulting in more code written as well as room for bugs and complications.\nTypeScripts static typing is not true static typing. Once TypeScript is compiled, it is transpiled into untyped Javascript, meaning we may still encounter bugs not found by the compiler. This may result in a false sense of security.\nConclusion Overall, we felt the type-checking benefits offered by Typescript outweighed the disadvantages during the development of Alexandria. Improved error-checking capabilities, readability, and the subsequent gains in team productivity made TypeScript the clear winner.\n","description":"","tags":null,"title":"Choosing TypeScript","uri":"/decisions/typescript/"},{"content":"Alexandria is primarily deployed as a web application, where users log in and interact with the information they have stored on the platform. Therefore, robust and scalable database choice is a must.\nOur research comparing between SQL and NoSQL databases centered around the following issues.\n   Parameter SQL NoSQL     Ability to handle high volume of complex queries ✅ ❌   Scalability Vertical Horizontal   Data type Structured Unstructured    SQL Structured Query Language (SQL) is the standard language for dealing with relational databases that define relationships in the form of tables. SQL databases works with a predefined schema, and can be used to effectively insert, search, update, and delete database records. SQL databases are table-based, and known for its vertical scalability.\nNoSQL NoSQL represents non-relational database management systems, where fixed schema is not required. It also avoids joins, and is easy to scale. NoSQL databases can be document based, key-value pairs, or graph databases. It uses dynamic schema for unstructured data, and is known for its ability to scale horizontally.\nChoosing SQL Our choice of SQL is based primarily on its ability to handle a high volume of complex queries. Each user interacting with the Alexandria may upload text, select words from the text to add translations to, change status of a word, or even switch language, all within the same session. Each action requires sometimes complex queries to complete.\nWhile SQL became Alexandria’s database of choice, we are acutely aware of the shortcomings of a SQL database, namely its vertical, versus horizontal scalability. Vertical scalability means that while we can increase the load on a single server by increasing RAM, CPU, or SSD, NoSQL databases handle more traffic by sharding and adding more servers, allowing it to become larger and preferable in the cases for large and ever-changing data sets.\n","description":"","tags":null,"title":"SQL or NoSQL?","uri":"/decisions/sql/"},{"content":"Alexandria is built based on the principles of service-oriented architecture [2]. In order to achieve readability, testability, and maintainbility, we ensured a separation of concern when it came to implementing the backend.\nMore specifically, we designed the backend server in three layers.\n   Input layer Logic Layer Data Access Layer     Responsible for accepting HTTP requests, validating for authentication and format, dispatch to correct logic function Algorithm which operates on the data in response to user input Reading and writing in-memory representation of records to and from database, performing complex queries over data    Based on this three-layer design including routes, services layer and data access layer, business logic is extracted away from the API.\nWhen a request is made to a route, the corresponding service is called, which in turn calls the data access layer. The data access layer queries the database and returns the result to the service layer, which in turn parses the data and pass it back to the route. The route can be kept “dumb”, and simply relays the data it receives from the service back to the client.\nThe separation of duties is particularly relevant when it comes to the service layer and the routing layer. The service layer should be framework agnostic, which encapsulates and abstracts business logic from the application. On the other hand, the routing layer is responsible for handling client requests and respond to them.\nWe capture the business logic and leverage the data access layer to interact with the database. Keeping concerns separate means that it does not handle request or response objects, does not respond to clients, provide anything related to the HTTP layer, nor does it interact directly with the database. Services are leveraged by passing the data needed instead of response or response objects.\nIn addition to keeping application logic out of the routes and data access layer, the three-layer approach also results in easier unit testing. Tests can now be designed to address each service function individually. Should we ever decided to change database implementation, most of the changes are confined to the data access layer.\n","description":"","tags":null,"title":"3 Layers for the Back End","uri":"/decisions/backend/"},{"content":"Project requirements In order to achieve both performance and user experience objectives for Alexandria, we settled on a number of goals that we’d like to fullfil at the initial stage.\nOne of the key features of the app is to reflect the changes made by a particular user interaction immediately, without an additional page reload.\nConsider the following user actions and their effects on a piece of text. Our challenge is to make every user interaction feel instantaneous and seamless.\n   User action Immediate Rendering     Click on word/phrase Translation input box available   Click on highlighted word/phrase Translation and status displayed   Update word/phrase status Highlighted colour on word changes    Another challenge is the ability of the UI to keep in sync with state changes of variables. With increased complexity, it becomes harder to track the exact state of the UI at every given moment during the lifecycle of the application. The challenge is to represent the UI accurately after a possible flurry of user interactions.\nPlain Javascript versus React Pitting plain JS against React against Alexandria’s initial requirements, React was the easy winner. Below is a summary of some of the differences between the two languages as it pertains to Alexandria.\nRendering efficiency With Javascript, UI is created in HTML on the server side, and sent to the browser. Each user interaction resulting in a data change requires an API call to the backend, and a re-rendering of the browser based on the new DOM. We briefly considered templating engines like Handlebars. While they are excellent choices for server-side rendering, they do not meet Alexandria’s goal of delivering speedy incremental updates of the DOM, event-handling, or frontend to backend communication.\nOn the other hand, React defines UI on the browser. An app starts with a blank container and loads the UI. The UI is defined by a component that returns the JSX - a HTML lookalike. The new component is rendered into the div container using the ReactDOM library, and the result will appear directly on the browser.\nBecause each UI is broken into smaller components, each component can render independently due to its use of the virtual DOM. One change in a component does not result in the re-rendering of the entire page, merely the changed component, thus improving the front-end user experience significantly.\nState management In JS, an event listener is attached to a DOM element to listen for changes. With each state change, we track the value deviation function, re-run the deviation functions and track the changes in return value. Those changed values are passed to the appropriate DOM element through the DOM API.\nWith React, the UI is set up to keep the entire state of a variable in the form of a “controlled component”, while an event (e.g. a button press) can be specified directly in the code, with no need for an event listener. The change to variable is registered, and the UI is updated automatically. There’s no need to go into the DOM to find the variable to update.\nBeing able to design components that hold state was fundamental to the design of Alexandria. Using React, each “word” residing within a piece of text can have it’s own state (translation, learning stage, context). When a user changes a word’s state, the result is displayed immediately on the DOM. In this case, the word would be highlighted, and its translation and context would be saved to the word’s state.\nReact’s implementation of hooks and component state fit perfectly with this requirement.\nMaintainability and Reusability In JS, the markup and functionality are kept separate. As apps grow bigger, this becomes a major source of complexity, where a developer needs to keep track of both sets of codes.\nIn React, the app is split into components, where each component maintains the code required to both display and updates the UI. This also allows reusability of components, another benefit.\nRecoil When it comes to choosing a state management tool, we began by assessing what the application needed.\nIt was immediately clear to us that users will be interacting intensively with highly dynamic components: highlighted words, translations, status, and languages are all expected to change with every user interaction. Therefore, it was vital that after each update, each component state is accurately captured and rendered back to the users in the most efficient manner.\nTurning our attention to state management tools available on the market, we quickly realized although the number of libraries and packages are numerous, most belong to one of three categories in addition to the React natively supported Context API: Flux (Redux, Zustand), Proxy (Mobx, Valtio), or Atomic (Recoil, Jotai) [3].\nWithin the context of Alexandria, we took a closer look at three choices: React’s Context API, Redux, and Recoil. Redux has long been the state management library of choice for applications of a certain size, while React’s very own Context API provided state management directly from React itself. Adding to the mix, new kid on the block Recoil had been released by Facebook, and touted as a React-specific state management library maximizes developer happiness.\nChoosing the right tool for the job took some careful examination. While we did not perform an exhaustive benchmarking study, some initial sandboxing helped us arrive at the following observations.\nContext API How it works A context provider is created on the parent component where the global state sits. Data is passed down the tree to its children components. Those children components are able to access the context object from the parent through props drilling.\nThis works great when the context data changes only occasionally. But the solution breaks down when we store substantive amount of data in the context. In the case of Alexandria, the context data would change every time the user interacts with one word or phrase in the text, and causes unnecessary re-rendering of all components of the provider, in this case, all of the words/phrases.\nAlternatively, the context object can be broken down into multiple states. This, however, introduces complexity into the codebase that could be avoided through another approach.\nPros  Context API is part of React’s built-in package, requiring no additional installation Relatively easy to set up and start working Great for static or rarely refreshed data  Cons:  Excessive unnecessary re-rendering affects performance  Redux How it works Instead of changing state by propagating values through the component tree, Redux takes a different approach. When a component state changes, the component dispatch an action to a reducer. The reducer handles the action (e.g. change a word state), manipulates the old state to reflect the new state. The reducer goes to the central store, which that manages all the states of the application. The application subscribes to part or all of the store, which then passes updated states as props to components that subscribe to it. This is how the application handles a state change.\nPros  Great for often-refreshed data, only listen to what has changed, only the value changed re-renders Integrates with react-redux library Server-side rendering possible  Cons  Much more complicated to get started, introduces substantial complexity to application  Recoil How it works Recoil was created to solve a number of challenges very specific to large and interactive apps, where issues of persistence and robustness are central. More specifically, how does one keep different branches of the tree in sync where components live in different branches of the tree? How does one avoid re-rendering multiple components when state change(s) are detected in the same provider?\nIn order to meet these global state management challenges, Recoil implemented small, shared units of state named “atoms”, and derived states named “selectors”. Atoms are changeable and subscribable pieces of state. That is to say, one particular atom can be subscribed to by components, and atoms can be used to derive data from state. States are stored within the React tree, similar to how useState and useContext hooks function, making it easy to get started. Recoil encourages separation of state into separate pieces in order to route components. The existence of atoms is particularly well-suited for applications that receive a high frequency of updates.\nPros  Easy to set up and implement, experience of working with useRecoilState similar to working with useState Re-renders only components with changed state, improves performance  Cons  Library still in beta  Choosing Recoil Recoil became our state management library of choice based on two key considerations. One, it delivered optimal performance by rendering only component that changes upon user interaction. Two, it offered maximal developer happiness. The simplicity of set up and usage made state management a non-issue, where state changes are central to the viability of Alexandria.\n","description":"","tags":null,"title":"React and Recoil","uri":"/decisions/react-recoil/"},{"content":"Database and API Having settled on a tech stack, we turn our attention to two vital aspects of backend architecture: database and API design.\nWith a multitude of objects to keep track of, we dive into the process of identifying tables and columns, their relationships with each other, and normalizing the resulting SQL database.\nOne key aspect of Alexandria’s initial goal is to provide an application that is highly interactive. This particular need is well served by a single page application. Working from those assumptions, it made sense to expose an API that allowed both our single page React app to talk to the server, as well as opening the door for alternative front ends at a future stage. A decoupling of front and end back allows not only for API calls made from, say, a mobile app later on, but also contributions from a different front end.\n","description":"","tags":null,"title":"Database and API","uri":"/database-api/"},{"content":"Normalization To normalize a database, we restructure it in accordance to a series of normal forms that avoids the redundancy of data, undesirable dependencies, and inconsistencies.\nIn his paper “Further Normalization of the Data Base Relational Model\" [4], Edgar F. Codd, the “father” of the relational database model, listed these objectives of normalization:\n To free the collection of relations from undesirable insertion, update and deletion dependencies. To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs. To make the relational model more informative to users. To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.  The following set of tables serves as an example for a database that is not properly normalized.\nTable one:    username word word_language translation translation_language     Mark house en Haus de   Dana house en maison fr   Eamon casa es house en    Table two:    id username email is_admin     1001 Dana dana@email.com true   1002 Eamon eamon@email.com true   1003 Mark marc@email.com false    Table one holds the values of four key entities: users, words, translations, and languages. Table two maintains user information.\nTwo independent username columns exist in two tables: an example of redundancy that can lead to so-called “update anomalies”. If we discover after the initial insertion that user with username Marc is not spelled with a “k” but with a “c”, and only proceed to change this data in the users table, then data across the database will become inconsistent. The username needs to be updated in two different places, making database updates open to errors.\nNormal forms Normalization of relational databases is guided by rules that build on each other called “normal forms”.\nTo satisfy the the first normal form (1NF), data sets must not contain tables themselves, and each data set must have a unique primary key. In the example above, with no nested tables and primary key for only the users table, 1NF is not satisfied.\nThe second normal form (2NF) must fulfill rules of the first normal form, and demands every attribute of a data set to be functionally dependent on the primary key. Attributes not dependent on the primary key belongs to their own table.\nThe third normal form (3NF) requires 2NF and requires the elimination of any transitive functional dependencies. For example, if B depends on primary key A, but C depends on B, then both B and C should be extracted to their own table.\nWithin the scope of Alexandria, our database was designed to satisfy 3NF, thus protecting it from insertion, update and deletion anomalies.\nTables We started out with a few iterations of basic design. This is the graphical representation of one the first drafts:\nAs our understanding and ambitions of Alexandria’s user needs and functionalities grew, the number of tables in our database grew. Here’s just a few examples of additional features we considered implementing for the MVP:\n User having access and choosing from other users' translations Saving every existing translation with a new context Users setting favourite dictionaries  Once we pared down Alexandria’s feature list to arrive at an MVP, our database complexities shrank. The reduced scope supports a single-user experience, simplified user profile and context handling, while laying the groundwork for cross-user content sharing.\nThis is what we arrived at for the initial release of Alexandria:\nIn the final design, we stripped the connecting tables of their own unique ids, and made composite keys - the combinations of the foreign key columns - the primary keys. This ensures that connections are unique and a user cannot have two statuses for the same word.\nWe left in id as the primary key for the words table while acknowledging it could have been possible to replace unique keys with composite keys - as every word is unique in its own language. Bar one table, Alexandria has achieved 3NF.\nNaming things As a useful side effect of designing the database early on, we had to come to a common understanding of naming entities in our code. For example, what is a word, and what is a phrase? Finding that common language was essential for communication going forward.\nAlexandria’s MVP database tables Main tables    table description     users The people using Alexandria. Each user can actively learn one language at a time and translate to one base language. If the language preferences change, previous progress (translations, word statuses) is kept.   admins The people administering Alexandria. That would be us for now.   languages The languages that are supported by Alexandria. For the first release, it is a list of ten. What they have in common is their use of a latin-based alphabet (to avoid surprises with our text parser for now) and their being supported by the PostgreSQL text search. A flag representing the language is saved as a unicode character combination to be used in menus across the application.   texts The key entity for learning. Texts are currently provided by users through forms, later through URLs and files. Texts default to being private which should particularly suit those who learn best by reading state secrets or erotic literature. Public texts, that can be used by every user to study, could be an option for a future release. The database already provides the necessary column.   words Translatable entities. Usually single words like cabbage, but also compounds, or phrases, like get up or l’hôtel. Each word must be unique within its language. It can have different meanings (translations) but in database terms, each combination of letters must occur only once per language.   translations Translations of words can be provided or selected by the users, but they exist independently of the users who created them. The go from the language of the word in question to a target language associated with this specific translation. Translations could also also be automatically generated to seed the database with a language’s most common words; another feature for a follow up version.   webdictionaries Online resources in which the user can look up a word or phrase. They    Connecting tables    table purpose     users_words Word statuses for a user-word combination are “learning”, familiar”, and “learned”.\nUser dictionaries could be generated using this table.   users_translations Every translated word originates in a text. Surrounding words from that text form the translation’s context. Users have their own contexts for every translation. If the translation is not newly provided but simply selected from previous translations, current context is saved and connected to selected the translation and user.   webdictionary_preference Users can have one favourite dictionary per source language.    ","description":"","tags":null,"title":"Database Architecture","uri":"/database-api/database/"},{"content":"During the design of Alexandria’s API, we aimed at following REST principles as outlined in Roy Fielding’s 2000 dissertation [3] within the constraints of our application in its current state of development.\n Client-Server: This separation came naturally with the decision to create a React-based single page application. Stateless: For security reasons and device interchangeability, only the user id is stored with the client in a JSON Web token that is sent with every request. Additional information is always drawn from the user state kept in a Recoil atom. The request includes the combined state information, and sends it to the server. Cacheable: The core of Alexandria, the single text and translation page is a highly dynamic affair where cached responses for word and translation queries could seriously damage the user experience. We decided to only explicitly allow caching of responses for resources that do not change often: list of languages, web dictionary list, and individual texts. Layered System: Front end and back end of Alexandria reside on different servers, allowing for independent scalability. Code-on-Demand: This was not actively implemented in Alexandria. Uniform Interface: According to Fielding, “In order to obtain a uniform interface, multiple architectural constraints are needed to guide the behavior of components: …  identification of resources, manipulation of resources through representations, self-descriptive messages, hypermedia as the engine of application state.”    While designing the API, significant effort was expended in getting the uniform interface right. We focused on refining a system of URLs (routes) that allows the clients or future contributors to retrieve data that is logical, predictable, and readable.\nHere is an overview of our API endpoints.\nResources and collections  Resources are the individual items (rows) of the main database tables (the once with a “single noun” name). Each resource has a unique id within its table. Collections are lists of resources, eg. words or texts, that can be filtered by criteria such as languages or user.  Route patterns Basic access  Type of the resource queried is always first part of the route. This is the case for both individual resources and collections. Queried resource is named in plural form. Individual resources are accessed by their id. Data from other resources associated with the queried resource might be sent back as well, but querying those happens on the backend and is not exposed via the API. Creation, updates and deletion of resources happens via the basic route, ie. without filters (see below).  Examples  GET /texts/ =\u003e a collection of all texts GET /texts/38422 =\u003e the text with id 38422 POST /translations =\u003e create a new translation PUT /users =\u003e update user data DELETE /texts/38422=\u003e removes text with id 22533  Filtering collections  If second part of the route is not an id, we are looking for a filtered collection of that resource. Filters are indicated by a keyword (in singular) and an unique identifier. Filters can be chained. Filtering a logged-in user is indicated via the URL. Filtering collections belonging to a user is done on the backend and based on the functionality and access rights of the app.  Examples  GET /words/text/17/language/fr =\u003e get all words from text 17 that have a translation to French (unique identifier fr) GET /texts/language/fr =\u003e all texts in French (unique identifier de) belonging to the current user  Special routes Certain functionality that the client demands from the server did not fit the resource schema outlined above, e.g. login, logout, or user related functions. In those cases, route URLs deviate from the pattern after the initial resource, and spell out what is expected of them.\n","description":"","tags":null,"title":"API Design","uri":"/database-api/api/"},{"content":"Challenges and Solutions Treanslating our technical decisions and research into implementation was where the pedal met the metal. We were faced with a number of challenges: from automated testing, text parsing, to phrase selection in the UI.\n","description":"","tags":null,"title":"Challenges and Solutions","uri":"/challenges/"},{"content":"One challenge we faced early on when implementing the backend and database, was how the testing of pull requests could be automated.\nDocker image to maintain test reliability After implementing the first few unit tests, we were confronted with the inefficiency of having to manually test routes using Postman when reviewing a PR. Given the project’s likely growth in both size and scope, this was not a sustainable solution.\nAnother challenge related to testing arose, this time related to maintaining the fidelity of the testing database. A good level of test coverage using Jest was achieved quickly, but with each team member using their own local testing database, the outcomes were not always reliable. To enforce testing reliability and control, the database needed to maintain a known state.\nTo achieve this objective, we implemented a PostgreSQL Docker image that would automatically spin up a Docker instance, set up and seed the database each time tests were run. One of the inherent advantages of using a Docker image is that unless explicitly set up, data does not persist after the Docker instance is shut down. We were able to take advantage of this property to avoid having to reset the database before running tests. This made the testing process more reliable and easier to control.\nGithub action to automating testing To review a pull request, a reviewer would download the PR branch, run the test suite manually to ensure there had been no regression. Given the frequency of code changes and speed of development, we decided to implement automated testing to speed up the testing processing, and make reviewers' job easier.\nAfter some investigation, we settled on implementing GitHub actions. We pointed automated tests on GitHub to the same PostgreSQL Docker image, seeded it with data as though we would using on our local machines. As Alexandria’s code was already hosted on Github, this greatly simplified the review process. As tests results became available, they were displayed prominently on the PR, and the team was notified immediately should a problem arose.\nThis workflow greatly simplified the pull request and review process, saving us significant time in catching new bugs or regression issues.\n","description":"","tags":null,"title":"Automating Tests","uri":"/challenges/automated-tests/"},{"content":"Finding words and phrases in text Our decision to work with a relational versus a document-based database gave us the flexibilty to perform complex queries. Our decision to use PostgreSQL specifically, brought another unexpected benefit: built-in full text search.\nOn the front end, Alexandria highlights all the words and phrases a user has already encountered. To find out whether a text contains words or phrases that the user has already marked, the words of the text have to be compared to all the user’s words and phrases saved in the database.\nWhile implementing a search algorithm using tries, a closer look at the PostgreSQL documentation yielded the discovery that the database provided full text search. Not only were the search functionalities just what we needed, implementing this solution provided a number of additional advantages over a solution of our own:\n Written in C, the database implementation to support search was likely faster than anything implemented in TypeScript. Since data would have been cleansed and filtered at the start of the chain, it was more efficient than having the entire data set transferred and processed by either server or client.  PostgreSQL text search works by parsing and normalizing a text into a text search vector of the tsvector data type. Search terms on the other hand, are parsed and normalized into text search queries of the tsquery type. The parser breaks a string (text or search term) into individual tokens, mostly words in our case.\nNormalization removes common small words from tokens, and reduces words to their stems according to language specific dictionaries. Heroku’s PostgreSQL installation came with support for twenty languages.\nWhile this normalization step is powerful, its default setting is geared for full text search, returning results including “handling” and “handlebar” when the original search term is “handle”. For a language learner, this was too lenient, as “handle” and “handling” are considered two distinct words. Luckily for our purposes, PostgreSQL also ships with a simple configuration that parses the text, but leaves the tokens as they are. This is the configuration implemented with Alexandria.\nTo save processing time, each text is parsed immediately into a tsvector when added to the database (or modified later). The vector is saved in a column of the texts table defined like this:\nALTER TABLE texts ADD COLUMN tsvector_simple tsvector GENERATED ALWAYS AS (to_tsvector('simple', title || ' ' || body)) STORED In a similar fashion, every word or phrase added to the database also gets a column in which the parsed tsquery is saved.\nWe find all the words in text 5 that user 1 has in their vocabulary with this SQL query:\nSELECT w.id, w.word, uw.word_status FROM words AS w JOIN users_words AS uw ON w.id = uw.word_id WHERE uw.user_id = 1 AND w.language_id = (SELECT t.language_id FROM texts AS t WHERE t.id = 5) AND w.tsquery_simple @@ (SELECT t.tsvector_simple FROM texts AS t WHERE t.id = 5); Parsing text into React and HTML Leveraging the prowess of PostgreSQL, a user’s texts and vocabulary list will make their way to the client. The front-end application kicks in at this stage to present these two sets of data, satisfying the following requirements:\n Every occurrence of a word from the list must be highlighted in the text according to its level of familiarity. Every occurrence of a phrases must be highlighted in the text according to its level of familiarity. Each word from the text must be clickable, regardless of whether a word is highlighted, or whether it is part of a phrase. Each phrase must be clickable/selectable. Punctuation and spaces must be part of any words (with the exception of the apostrophe and hyphen). The text must be split into sentences to facilitate saving the context of a word when adding a new translation. Bonus: Phrases in the userWords list should be highlighted if found in text, regardless of whether phrase in the list or phrase in the text contains punctuation.  Due to the way the browser DOM handles click events, it was clear words would end up nested in \u003cspan\u003es. A phrase meant another set of \u003cspan\u003es around a word \u003cspan\u003es . Splitting the text at the whitespace characters did not suffice, since punctuation stuck to their preceding word. We approached the problem with regular expressions.\nZooming in from the full text body down to individual words, text is parsed and split up into React components and HTML elements in these steps.\n  The text body is divided into Paragraph components, using the split string method.\n  Paragraphs are split into Sentence components using this regular expression:\n/[^\\s]([^!?.:;]|\\.{3})*[\"!?.:;\\s]*/g This expression splits sentences at ! ? . : ; but not at an ellipsis ...\n  Sentences are split into tokens that are either phrases and words, or spaces and punctuation.\nCorresponding regular expression matches 1) all phrases found on the list, 2) any words, 3) non-words (ie. spaces and punctuation).\nHere is an example with two phrases:\n/of course|get up|(?\u003cwords\u003e[\\p{L}\\p{M}\\'-]+)|(?\u003cnowords\u003e[^\\\\p{L}\\\\p{M}\\'-]+)/gui The Unicode character class \\p is utilized with categories Letter {L} and Mark {M} to account for letters beyond standard ASCII characters.\n  To satisfy the bonus criterion of finding phrases with any punctuation in them, phrases at the beginning need to be replaces with:\n/of[^\\\\p{Letter}\\\\p{Mark}\\'-]+course|get[^\\\\p{Letter}\\\\p{Mark}\\'-]+up/ to allow punctuation or spaces between words that make up the phrase.\nAdditionally, in preparation for parsing all phrases from the users vocabulary, punctuation is stripped off of words.\n  If a token is a phrase (simply checking for spaces suffices), it will be handed over to a Phrase component. Words go into the Word components, and punctuation and spaces are wrapped into \u003cspan\u003es. That is necessary for selecting phrases from the text (see next section).\n  In a Phrase component, phrase are wrapped into \u003cspan\u003es, and the parsing process from steps 3 and 5 is repeated without phrases.\n  Finally, the Word component wraps \u003cspan\u003es around a word.\n  The CSS for highlighting the words is applied in steps 6 and 7.\n","description":"","tags":null,"title":"Text Parsing","uri":"/challenges/text-parsing/"},{"content":"MVP Confronted with the fully-fledged feature set of commercial language learning applications, our team made the early decision to create a minimum viable product that reflects the essence of what we find crucial in such a product.\n User experience: mobile-first, fast and seamless processing of user interactions Backend and API: layered, robust, RESTful, maintainable, and flexible enough to service front-end needs Database: normalized and withstands scale CI/CD: establish continuous deployment pipelines  Moving Forward Our core team has plans to continue support the maintenance and development of Alexandria.\nBackend  API enhancements.  Frontend   Full user access and ownership control:\n Add full suite of user access tools, including email validation upon sign-up and password reset. Offer users full control, where words and translations can be deleted as well as updated. User are currently only able to access texts and translations based on their own inputs. Going forward, users will have the choice of accessing a global pool of shared texts and translations, further reducing usage friction, take advantage of other users' contribution, and making the experience more dynamic.    Starter kit upon sign-up: Offer new users a package of existing texts to improve onboarding experience\n  Import/export: Import / export functionality for interoperability with other learning tools\n  Additional language support: Alexandria currently supports 10 languages. We would like to expand the number of languages supported after additional testing.\n  Admin dashboard: Allows admins to make changes to user accounts from the front-end.\n  Mobile optimization: Additional development to further enhance user mobile experiences.\n  Testing  Alexandria backend currently has close to full coverage testing in place. Front end has some unit testing and Cypress testing in place. We plan on introducing testing suite that offers full coverage testing for the front end.  ","description":"","tags":null,"title":"MVP and Future Plans","uri":"/future/"},{"content":"Alexandria’s ability to allow users select and save phrases along with its translation is one of the key features that set her apart from other open-source or commercial products on the market. This particular functionality was challenging to implement due to the way texts are displayed.\nEach word in a user text has its own state, and resides within its own span. When user clicks on a word, it is added to the currentWord state, triggering a re-render of the translation input component, which contains the current word, existing translations, links to dictionaries and translations, as well as the option for user to set their level of familiarity with the word.\nPhrases on the other hand, are components separate from words. They are also surrounded by highlighted spans based on a user’s level of familiarity with the phrase. We needed a way to create phrases on the fly as selected. When a user selects several words, their selection is added to the userWords object, which automatically highlights the new phrase.\nDepending on how the user selected the word, this seemingly simple task became challenging.\nConsider, for example, the phrase “for example”.\nIf a user started or ended their selection in the middle of a word, the currentWord might end up being displayed as “or examp” instead of “for example”.\nUsing the browser API Selection, we were able to leverage anchorNode and focusNode to access where the selection started and ended. With access to those nodes, we could extract full words using a node’s textContent property. The task of replacing first and last words of a selection with the full words became trivial. This resulted in an improved user experience: users can now select phrases without having to carefully point the cursor to select full words.\nHowever, if a user selected a phrase by dragging the mouse from right to left, the first and last word would be reversed. To account for this user behaviour, we saved the X location of the mouse pointer onMouseDown, and compared it to the end X location onMouseUp to see if the selection was made backwards.\nTesting our implementation, we discovered that when a selection was made across multiple lines with a start point further left than the end point, this approach failed. But since we had access to the context (the phrase the selection was made from), we could test if the phrase appeared in the context. If that is not the case, the first and last words were switched, thus greatly simplifying the selection logic.\n","description":"","tags":null,"title":"Phrase selection UI","uri":"/challenges/phrase-selection/"},{"content":" Stephen Krishan (2021): Explorations in Language Acquisition and Use Software Development Community (2019): What Is Service-Oriented Architecture? Yaroslav Lapin (2021) : Jotai vs. Recoil: What are the differences? Edgar F. Codd (1972): Further Normalization of the Data Base Relational Model Roy Fielding (2000): Architectural Styles and the Design of Network-based Software Architectures  ","description":"","tags":null,"title":"References","uri":"/references/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"}]