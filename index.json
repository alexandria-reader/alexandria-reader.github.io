[{"content":"Current “learning with text” software applications fall under two broad categories.\nOne category includes long-standing open source projects that are well received but sparsely maintained, or newer free applications that started with a bang, but were quickly forgotten and subsequently neglected.\nThe second group of applications are commercial solutions that address the goal of language learning with text, but suffer from feature bloat, lack of user friendliness, or mobile performance issues.\nAlexandria would like to become the go-to solution for language learning enthusiasts. Through keeping the product free and open source, we aspire to continue developing product features that are crucial to language learners, while maintaining a joyful user experience based on the latest technological offerings.\n    Existing Free/Open Source Software Alexandria Commercial Products     Interactive front-end user experience ❌ ✅ ✅   Open Source ✅ ✅ ❌   Ongoing Development ❌ ✅ ✅   Mobile First ❌ ✅ ✅   Responsiveness to Community Input ❌ ✅ ✅   Phrase Selection ❌ ✅ ❓    Our main objectives at the initial stage are to: create a modern, fast, open source webapp with a mobile first interface. This will allow users to quickly and easily start learning without struggles with the interface, or excessive wait times for content to load.\n","description":"","tags":null,"title":"Competitor Analysis","uri":"/competitors/"},{"content":"Technical Decisions Every project starts with a number of technical decisions, Alexandria is no different. From programming language, database selection, to front-end library, a number of conscious choices were made to match the scope and demands of the application.\n","description":"","tags":null,"title":"Technical Decisions","uri":"/decisions/"},{"content":"Since its inception, TypeScript has been touted as a supercharged version of JavaScript that has the ability to spot common errors through a typed system, making the application more robust throughout its development.\nPrior to developing Alexandria, we looked into TypeScript as a feasible substitute to JavaScript.\nStatic typing JavaScript is dynamically typed. Therefore, it is not aware of variable type before instantiation at run time. TypeScript, on the other hand, is statically typed. Once an object’s type is declared, will only take certain values and can no longer be changed. Adding type support to JavaScript allows Typescript to catch type errors during compilation. Bugs caused by false assumptions of variable type are potentially eradicated.\nSuppose we want to create an object called UserWord that represents how a user is saved to the database by adding a translation and status.\ntype UserWord = { id?: number, word: string, status?: Status, translations: Array\u003cTranslation\u003e, languageId?: string }; UserWord has 5 properties with predefined types. If languageId was added as a number type, TypeScript would immediately warn us.\nAn even better example is the translations property, which takes an array of Translation type objects. If an attempt was made to add an object not of type Translation, we would again be warned by the compiler.\nIf an attempt was made to add a property not yet declared on the type, for example, language, the compiler would warn us that the property language does not exist on type UserWord.\nCompiler warnings could alert us to the existence of bugs ranging from type-related bugs to misspelled variables, before the code was even run. Over the course of development, this could save significant time and frustration, as far fewer errors would make it into the codebase.\nCode readability TypeScript has the potential to increases code readability.\nLet’s look at a function that takes an object as a parameter. In JavaScript, a developer would have to be familiar with the object and its properties, or possibly log the object to the console, in order to see what properties exist on the object. With TypeScript in VSCode, a developer can simply mouse over the type declaration to see a list of the properties that object will have, as well as the each property type.\nEasy refactoring Validating connections between different parts of a project is time consuming and error-prone, especially when one needs to refactor. By providing instant feedback during development, TypeScript makes refactoring a breeze. For example, if a function name is changed in one file but not in another, the TypeScript compiler will immediately alert developer to the issue, showing the exact file and line of code where the problem resides. This allows developers feel more confident in the code, and saves considerable time in ensuring no part of the project became broken during refactoring.\nFuture advantages As the codebase grows, having knowledge over the properties of an object increases code stability, and makes it easier to onboard future contributors to Alexandria.\nDisadvantages of TypeScript Although TypeScript is a productivity booster when used correctly, there are disadvantages.\nThe typing system can be difficult to use properly, resulting in more code written as well as room for bugs and complications.\nTypeScripts static typing is not true static typing. Once TypeScript is compiled, it is transpiled into untyped JavaScript, meaning we may still encounter bugs not found by the compiler. This may result in a false sense of security.\nConclusion Overall, we felt the type-checking benefits offered by TypeScript outweighed the disadvantages during the development of Alexandria. Improved error-checking capabilities, readability, and the subsequent gains in team productivity made TypeScript the clear winner.\n","description":"","tags":null,"title":"Choosing TypeScript","uri":"/decisions/typescript/"},{"content":"Alexandria is primarily deployed as a web application, where users log in and interact with the information they have stored on the platform. Therefore, robust and scalable database choice is a must.\nOur research comparing between SQL and NoSQL databases centred around the following issues.\n   Parameter SQL NoSQL     Ability to handle high volume of complex queries ✅ ❌   Scalability Vertical Horizontal   Data type Structured Unstructured    SQL Structured Query Language (SQL) is the standard language for dealing with relational databases that define relationships in the form of tables. SQL databases works with a predefined schema, and can be used to effectively insert, search, update, and delete database records. SQL databases are table-based, and known for its vertical scalability.\nNoSQL NoSQL represents non-relational database management systems, where fixed schema is not required. It also avoids joins, and is easy to scale. NoSQL databases can be document based, key-value pairs, or graph databases. It uses dynamic schema for unstructured data, and is known for its ability to scale horizontally.\nChoosing SQL Our choice of SQL is based primarily on its ability to handle a high volume of complex queries. Each user interacting with the Alexandria may upload text, select words from the text to add translations to, change status of a word, or even switch language, all within the same session. Each action requires sometimes complex queries to complete.\nWhile SQL became Alexandria’s database of choice, we are acutely aware of the shortcomings of a SQL database, namely its vertical, versus horizontal scalability. Vertical scalability means that while we can increase the load on a single server by increasing RAM, CPU, or SSD, NoSQL databases handle more traffic by sharding and adding more servers, allowing it to become larger and preferable in the cases for large and ever-changing data sets.\n","description":"","tags":null,"title":"SQL or NoSQL?","uri":"/decisions/sql/"},{"content":"Alexandria is built based on the principles of service-oriented architecture [2]. In order to achieve readability, testability, and maintainability, we ensured a separation of concerns when it came to implementing the back end.\nMore specifically, we designed the back-end server in three layers.\n   Input layer Logic Layer Data Access Layer     Responsible for accepting HTTP requests, authentication, validating format, dispatch to correct logic function Algorithm operates on the data in response to user input Reading and writing in-memory representation of records to and from database, performing complex queries over data    Based on this three-layer design including routes, services layer and data access layer, business logic is extracted away from the API.\nWhen a request is made to a route, the corresponding service is called, which in turn calls the data access layer. The data access layer queries the database and returns the result to the service layer, which in turn parses the data and pass it back to the route. The route can be kept “dumb”, and simply relays the data it receives from the service back to the client.\nThe separation of duties is particularly relevant when it comes to the service layer and the routing layer. The service layer is framework agnostic, which encapsulates and abstracts business logic from the application. On the other hand, the routing layer is responsible for handling client requests and responding to them.\nWe capture the business logic and leverage the data access layer to interact with the database. Keeping concerns separate means that it does not handle request or response objects, does not respond to clients, provide anything related to the HTTP layer, nor does it interact directly with the database. Services are leveraged by passing the data needed instead of response or response objects.\nIn addition to keeping application logic out of the routes and data access layers, the three-layer approach also results in easier unit testing. Tests are designed to address each service function individually. Should we ever decide to change Alexandria’s database implementation, most of the changes will be confined to the data access layer.\n","description":"","tags":null,"title":"3 Layers for the Back End","uri":"/decisions/backend/"},{"content":"Project requirements In order to achieve both performance and user experience objectives for Alexandria, we settled on a number of goals to fullfil at the initial stages.\nOne of the key features of the app is to reflect the changes made by a particular user interaction immediately, without an additional page reload.\nConsider the following user actions and their effects on a piece of text. Our challenge is to make every user interaction feel instantaneous and seamless.\n   User action Immediate Rendering     Click on word/phrase Translation input box available   Click on highlighted word/phrase Translation and status displayed   Update word/phrase status Highlighted colour on word changes    Another challenge is the ability of the UI to keep in sync with state changes of variables. With increased complexity, it becomes harder to track the exact state of the UI at every given moment during the lifecycle of the application. The challenge is to represent the UI accurately after a possible flurry of user interactions.\nPlain JavaScript versus React Pitting plain JS and React against Alexandria’s initial requirements, React was the easy winner. Below is a summary of some of the differences between the two languages pertaining to Alexandria.\nRendering efficiency With JavaScript, UI is created in HTML on the server side, and sent to the browser. Each user interaction resulting in a data change requires an API call to the back end, and a re-rendering of the browser based on the new DOM. We briefly considered templating engines like Handlebars. While they are excellent choices for server-side rendering, they do not meet Alexandria’s goal of delivering speedy incremental updates of the DOM, event-handling, or front end to back end communication.\nOn the other hand, React defines UI on the browser. An app starts with a blank container and loads the UI. The UI is defined by a component that returns the JSX - a HTML lookalike. The new component is rendered into the div container using the ReactDOM library, and the result will appear directly on the browser.\nBecause each UI is broken into smaller components, each component can render independently due to its use of the virtual DOM. One change in a component does not result in the re-rendering of the entire page, merely the changed component, thus improving the front-end user experience significantly.\nState management In JS, an event listener is attached to a DOM element to listen for changes. With each state change, we track the value deviation function, re-run the deviation functions and track the changes in return value. Those changed values are passed to the appropriate DOM element through the DOM API.\nWith React, the UI is set up to keep the entire state of a variable in the form of a “controlled component”, while an event (e.g. a button press) can be specified directly in the code, with no need for an event listener. The change to variable is registered, and the UI is updated automatically. There’s no need to go into the DOM to find the variable to update.\nBeing able to design components that hold state was fundamental to the design of Alexandria. Using React, each “word” residing within a piece of text can have its own state (translation, learning stage, context). When a user changes a word’s state, the result is displayed immediately on the DOM. In this case, the word would be highlighted, and its translation and context would be saved to the word’s state.\nReact’s implementation of hooks and component state fit perfectly with this requirement.\nMaintainability and Reusability In JS, the markup and functionality are kept separate. As apps grow bigger, this becomes a major source of complexity, where a developer needs to keep track of both sets of codes.\nIn React, the app is split into components, where each component maintains the code required to both display and updates the UI. This also allows reusability of components, another benefit.\nRecoil When it comes to choosing a state management tool, we began by assessing what the application needed.\nIt was immediately clear to us that users will be interacting intensively with highly dynamic components: highlighted words, translations, status, and languages are all expected to change with every user interaction. Therefore, it was vital that after an update, each component state is accurately captured and rendered back to the users in the most efficient manner.\nTurning our attention to state management tools available on the market, we quickly realized although the number of libraries and packages are numerous, most belong to one of three categories in addition to the React natively supported Context API: Flux (Redux, Zustand), Proxy (Mobx, Valtio), or Atomic (Recoil, Jotai) [3].\nWithin the context of Alexandria, we took a closer look at three choices: React’s Context API, Redux, and Recoil. Redux has long been the state management library of choice for applications of a certain size, while React’s very own Context API provided state management directly from React itself. Adding to the mix, new kid on the block Recoil had been released by Facebook, and touted as a React-specific state management library that maximizes developer happiness.\nChoosing the right tool for the job took some careful examination. While we did not perform an exhaustive benchmarking study, some initial sandboxing helped us arrive at the following observations.\nContext API How it works A context provider is created on the parent component where the global state sits. Data is passed down the tree to its children components. Those children components are able to access the context object from the parent through props drilling.\nThis works great when the context data changes only occasionally. But the solution breaks down when we store substantive amount of data in the context. In the case of Alexandria, the context data would change every time the user interacts with one word or phrase in the text, and causes unnecessary re-rendering of all components of the provider, in this case, all of the words/phrases.\nAlternatively, the context object can be broken down into multiple states. This, however, introduces complexity into the codebase that could be avoided through another approach.\nPros  Context API is part of React’s built-in package, requiring no additional installation Relatively easy to set up and start working Great for static or rarely refreshed data  Cons:  Excessive unnecessary re-rendering affects performance  Redux How it works Instead of changing state by propagating values through the component tree, Redux takes a different approach. When a component state changes, the component dispatch an action to a reducer. The reducer handles the action (e.g. change a word state), manipulates the old state to reflect the new state. The reducer goes to the central store, which that manages all the states of the application. The application subscribes to part or all of the store, which then passes updated states as props to components that subscribe to it. This is how the application handles a state change.\nPros  Great for often-refreshed data, only listen to what has changed, only the value changed re-renders Integrates with react-redux library Server-side rendering possible  Cons  Much more complicated to get started, introduces substantial complexity to application  Recoil How it works Recoil was created to solve a number of specific challenges faced by large and/or interactive apps, where issues of persistence and robustness are central. More specifically, how does one keep different branches of the tree in sync where components live in different branches of the tree? How does one avoid re-rendering multiple components when state change(s) are detected in the same provider?\nIn order to meet these global state management challenges, Recoil implemented small, shared units of state named “atoms”, and derived states named “selectors”. Atoms are changeable and subscribable pieces of state. That is to say, one particular atom can be subscribed to by components, and atoms can be used to derive data from state. States are stored within the React tree, similar to how useState and useContext hooks function, making it easy to get started. Recoil encourages separation of state into separate pieces in order to route components. The existence of atoms is particularly well-suited for applications that receive a high frequency of updates.\nPros  Easy to set up and implement, experience of working with useRecoilState similar to working with useState Re-renders only components with changed state, improves performance  Cons  Library still in beta  Choosing Recoil Recoil became our state management library of choice based on two key considerations. One, it delivered optimal performance by rendering only component that changes upon user interaction. Two, its simplicity offered maximal developer happiness. The ease of set up and usage made state management a non-issue, where state changes are central to the viability of Alexandria.\n","description":"","tags":null,"title":"React and Recoil","uri":"/decisions/react-recoil/"},{"content":"Database and API Having settled on a tech stack, we turn our attention to two vital aspects of back-end architecture: database and API design.\nWith a multitude of objects to keep track of, we dived into the process of identifying tables and columns, their relationships with each other, and normalizing the resulting SQL database.\nOne key aspect of Alexandria’s initial goal is to provide an application that is highly interactive. This particular need is well served by a single page application. Working from those assumptions, it made sense to expose an API that allowed both our single page React app to talk to the server, as well as opening the door for alternative front ends at a future stage. A decoupling of front and end back allows not only for API calls made from, say, a mobile app later on, but also contributions from a different front end.\n","description":"","tags":null,"title":"Database and API","uri":"/database-api/"},{"content":"Normalization To normalize a database, we restructured it in accordance to a series of normal forms that avoids the redundancy of data, undesirable dependencies, and inconsistencies.\nIn his paper “Further Normalization of the Data Base Relational Model\" [4], Edgar F. Codd, the “father” of the relational database model, listed these objectives of normalization:\n To free the collection of relations from undesirable insertion, update and deletion dependencies. To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs. To make the relational model more informative to users. To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.  The following set of tables serves as an example for a database that is not properly normalized.\nTable one:    username word word_language translation translation_language     Mark house en Haus de   Dana house en maison fr   Eamon casa es house en    Table two:    id username email is_admin     1001 Dana dana@email.com true   1002 Eamon eamon@email.com true   1003 Mark marc@email.com false    Table one holds the values of four key entities: users, words, translations, and languages. Table two maintains user information.\nTwo independent username columns exist in two tables: an example of redundancy that can lead to so-called “update anomalies”. If we discover after the initial insertion that user with username Marc is not spelled with a “k” but with a “c”, and only proceed to change this data in the users table, then data across the database will become inconsistent. The username needs to be updated in two different places, making database updates open to errors.\nNormal forms Normalization of relational databases is guided by rules that build on each other called “normal forms”.\nTo satisfy the the first normal form (1NF), data sets must not contain tables themselves, and each data set must have a unique primary key. In the example above, with no nested tables and primary key for only the users table, 1NF is not satisfied.\nThe second normal form (2NF) must fulfil rules of the first normal form, and demands every attribute of a data set to be functionally dependent on the primary key. Attributes not dependent on the primary key belongs to their own table.\nThe third normal form (3NF) requires 2NF and requires the elimination of any transitive functional dependencies. For example, if B depends on primary key A, but C depends on B, then both B and C should be extracted to their own table.\nWithin the scope of Alexandria, our database was designed to satisfy 3NF, thus protecting it from insertion, update and deletion anomalies.\nTables We started out with a few iterations of basic design. This is one of the first drafts:\nAs our understanding and ambitions of Alexandria’s user needs and functionalities grew, the number of tables in our database grew. Here’s a few examples of additional features we considered implementing for the MVP:\n User having access and choosing from other users' translations Saving every existing translation with a new context Users setting favourite dictionaries  Once we pared down Alexandria’s feature list to arrive at a MVP, our database complexities shrank. The reduced scope supports a single-user experience, simplified user profile and context handling, while laying the groundwork for cross-user content sharing.\nThis is what we arrived at for the initial release of Alexandria:\nIn the final design, we stripped the connecting tables of their own unique ids, and made composite keys - the combinations of foreign key columns - the primary keys. This ensures that connections are unique and a user cannot have two statuses for the same word.\nWe left in id as the primary key for the words table while acknowledging it could have been possible to replace unique keys with composite keys - as every word is unique in its own language. Bar one table, Alexandria has achieved 3NF.\nNaming things As an useful side effect of designing the database early on, we were forced to come to a common understanding of naming entities in our code. For example, what is a word, and what is a phrase? Finding that common language was essential for communication going forward.\nAlexandria’s MVP database tables Main tables    table description     users The people using Alexandria. Each user can actively learn one language at a time, and translate to one base language. If language preferences change, previous progress (translations, word statuses) is kept.   admins The people administering Alexandria. That would be the core development team for now.   languages The languages that are supported by Alexandria. For the first release, it is a list of ten. What they have in common is their use of a Latin-based alphabet (to avoid surprises with our text parser), in addition to language support by PostgreSQL text search. A flag representing the language is saved as a unicode character combination to be used in menus across the application.   texts The key entity for learning. Texts are currently provided by users through forms, later through URLs and files. Texts default to private, which should particularly suit those who learn best by reading state secrets or erotic literature. Public texts accessible to every user is on the roadmap for a future release. Database is created with necessary column to provide this functionality.   words Translatable entities. Usually single words like cabbage, but also compounds, or phrases, like get up or l’hôtel. Each word must be unique within its language. It can have different meanings (translations) but in database terms, each combination of letters occurs once per language.   translations Translations of words can be provided or selected by the users, but they exist independently of the users who created them. Since a word is aware of the language it’s in, the translation table tracks only the language a word is translated to, in column target_language. Seeding the database with a language’s most common words and translations could be another future release feature.   webdictionaries Online resources where user can look up a word or phrase.    Connecting tables    table purpose     users_words Word statuses for a user-word combination are “learning”, familiar”, and “learned”.\nUser dictionaries could be generated using this table.   users_translations Every translated word originates in a text. Surrounding words from that text form the translation’s context. Users have their own contexts for every translation. If the translation is not newly provided but simply selected from previous translations, current context is saved and connected to the current translation and user.   webdictionary_preference Users can have one favourite dictionary per source language.    ","description":"","tags":null,"title":"Database Architecture","uri":"/database-api/database/"},{"content":"During the design of Alexandria’s API, we aimed at following REST principles [3] as outlined in Roy Fielding’s 2000 dissertation. Within the constraints of our application in its current state of development, we arrived at the following.\nClient-Server This separation came naturally with the decision to create a React-based single page application.\nStateless For security reasons and device interchangeability, only the user id is stored with the client in a JSON Web token, sent with every request. Additional information is drawn from a Recoil atom that keeps track of the user state. The request includes the combined state information, and sends it to the server.\n**Cacheable*v The core of Alexandria - the single text and translation page, is a highly dynamic affair, where cached responses for word and translation queries could seriously damage the user experience. We explicitly allowed the caching of responses only for resources that do not change often: list of languages, web dictionary list, and individual texts.\nLayered System Front and back end of Alexandria reside on different servers, making scalability independent of each other.\nCode-on-Demand This was not actively implemented in Alexandria.\nUniform Interface According to Fielding, “In order to obtain a uniform interface, multiple architectural constraints are needed to guide the behavior of components: …\n identification of resources, manipulation of resources through representations, self-descriptive messages, hypermedia as the engine of application state.”  During API development, significant effort was expended in getting the uniform interface right. We focused on refining a system of URLs (routes) that allows the clients or future contributors to retrieve data that is logical, predictable, and readable.\nHere is an overview of our API endpoints.\nResources and collections  Resources are the individual items (rows) of the main database tables (the once with a “single noun” name). Each resource has a unique id within its table. Collections are lists of resources, eg. words or texts, that can be filtered by criteria such as languages or user.  Route patterns Basic access  Type of the resource queried is always first part of the route. This is the case for both individual resources and collections. Queried resource is named in plural form. Individual resources are accessed by their id. Data from other resources associated with the queried resource might be sent back as well, but querying those happens on the back end and is not exposed via the API. Creation, updates and deletion of resources happens via the basic route, ie. without filters (see below).  Examples  GET /texts/ =\u003e a collection of all texts GET /texts/38422 =\u003e the text with id 38422 POST /translations =\u003e create a new translation PUT /users =\u003e update user data DELETE /texts/38422=\u003e removes text with id 22533  Filtering collections  If the second part of a route is not an id, we are looking for a filtered collection of that resource. Filters are indicated by a keyword (in singular) and an unique identifier. Filters can be chained. Filtering a logged-in user is indicated via the URL. Filtering collections belonging to a user is done on the back end and based on the functionality and access rights of the app.  Examples  GET /words/text/17/language/fr =\u003e get all words from text 17 that have a translation to French (unique identifier fr) GET /texts/language/fr =\u003e all texts in French (unique identifier de) belonging to the current user  Special routes Certain functionality that the client demands from the server did not fit the resource schema outlined above, e.g. login, logout, or user related functions. In those cases, route URLs deviate from the pattern after the initial resource, and spell out what is expected of them.\n","description":"","tags":null,"title":"API Design","uri":"/database-api/api/"},{"content":"Challenges and Solutions Translating our technical decisions and research into implementation was where the pedal met the metal. We were faced with a number of challenges: from automated testing, text parsing, to phrase selection in the UI.\n","description":"","tags":null,"title":"Challenges and Solutions","uri":"/challenges/"},{"content":"One challenge we faced early on when implementing the back end and database, was how the testing of pull requests could be automated.\nDocker image to maintain test reliability After implementing the first few unit tests, we were confronted with the inefficiency of having to manually test routes using Postman when reviewing a PR. Given the project’s likely growth in both size and scope, this was not a sustainable solution.\nAnother challenge related to testing arose, this time related to maintaining the fidelity of the testing database. A good level of test coverage using Jest was achieved quickly, but with each team member using their own local testing database, the outcomes were not always reliable. To enforce testing reliability and control, the database needed to maintain a known state.\nTo achieve this objective, we implemented a PostgreSQL Docker image that would automatically spin up a Docker instance, set up and seed the database each time tests were run. One of the inherent advantages of using a Docker image is that unless explicitly set up, data does not persist after the Docker instance is shut down. We were able to take advantage of this property to avoid having to reset the database before running tests. This made the testing process more reliable and easier to control.\nGithub action to automating testing To review a pull request, a reviewer would download the PR branch, run the test suite manually to ensure there had been no regression. Given the frequency of code changes and speed of development, we decided to implement automated testing to speed up the process, and make reviewers' job easier.\nAfter some investigation, we settled on implementing GitHub Actions. We pointed automated tests on GitHub to the same PostgreSQL Docker image, seeded it with data as we would using on our local machines. As Alexandria’s code was already hosted on Github, this greatly simplified the review process. As tests results became available, they were displayed prominently on the PR, and the team was notified immediately should a problem arise.\nThis workflow greatly simplified the pull request and review process, saving us significant time in catching new bugs or dealing with regression issues.\n","description":"","tags":null,"title":"Automating Tests","uri":"/challenges/automated-tests/"},{"content":"Finding words and phrases in text Our decision to work with a relational versus a document-based database gave us the flexibility to perform complex queries. Our decision to use PostgreSQL specifically, brought another unexpected benefit: built-in full text search.\nOn the front end, Alexandria highlights all the words and phrases a user has already encountered. To find out whether a text contains words or phrases that the user has already marked, the words of the text have to be compared to all the user’s words and phrases saved in the database.\nWhile implementing a search algorithm using tries, a closer look at the PostgreSQL documentation yielded the discovery that the database provided full text search. Not only were the search functionalities just what we needed, implementing this solution provided a number of additional advantages over a solution of our own:\n Written in C, the database implementation to support search was likely faster than anything implemented in TypeScript. Since data would have been cleansed and filtered at the start of the chain, it was more efficient than having the entire data set transferred and processed by either server or client.  PostgreSQL text search works by parsing and normalizing a text into a text search vector of the tsvector data type. Search terms on the other hand, are parsed and normalized into text search queries of the tsquery type. The parser breaks a string (text or search term) into individual tokens, mostly words in our case.\nNormalization removes common small words from tokens, and reduces words to their stems according to language specific dictionaries. Heroku’s PostgreSQL installation came with support for twenty languages.\nWhile this normalization step is powerful, its default setting is geared for full text search, returning results including “handling” and “handlebar” when the original search term is “handle”. For a language learner, this was too lenient, as “handle” and “handling” are considered two distinct words. Luckily for our purposes, PostgreSQL also ships with a simple configuration that parses the text, but leaves the tokens as they are. This is the configuration implemented with Alexandria.\nTo save processing time, each text is parsed immediately into a tsvector when added to the database (or modified later). The vector is saved in a column of the texts table defined like this:\nALTER TABLE texts ADD COLUMN tsvector_simple tsvector GENERATED ALWAYS AS (to_tsvector('simple', title || ' ' || body)) STORED In a similar fashion, every word or phrase added to the database also gets a column in which the parsed tsquery is saved.\nWe find all the words in text 5 that user 1 has in their vocabulary with this SQL query:\nSELECT w.id, w.word, uw.word_status FROM words AS w JOIN users_words AS uw ON w.id = uw.word_id WHERE uw.user_id = 1 AND w.language_id = (SELECT t.language_id FROM texts AS t WHERE t.id = 5) AND w.tsquery_simple @@ (SELECT t.tsvector_simple FROM texts AS t WHERE t.id = 5); Parsing text into React and HTML Leveraging the prowess of PostgreSQL, a user’s texts and vocabulary list will make their way to the client. The front-end application kicks in at this stage to present these two sets of data, satisfying the following requirements:\n Every occurrence of a word from the list must be highlighted in the text according to its level of familiarity. Every occurrence of a phrases must be highlighted in the text according to its level of familiarity. Each word from the text must be clickable, regardless of whether a word is highlighted, or whether it is part of a phrase. Each phrase must be clickable/selectable. Punctuation and spaces must be part of any words (with the exception of the apostrophe and hyphen). The text must be split into sentences to facilitate saving the context of a word when adding a new translation. Bonus: Phrases in the userWords list should be highlighted if found in text, regardless of whether phrase in the list or phrase in the text contains punctuation.  Due to the way the browser DOM handles click events, it was clear words would end up nested in \u003cspan\u003es. A phrase meant another set of \u003cspan\u003es around a word \u003cspan\u003es . Splitting the text at the whitespace characters did not suffice, since punctuation stuck to their preceding word. We approached the problem with regular expressions.\nZooming in from the full text body down to individual words, text is parsed and split up into React components and HTML elements in these steps.\n  The text body is divided into Paragraph components, using the split string method.\n  Paragraphs are split into Sentence components using this regular expression:\n/[^\\s]([^!?.:;]|\\.{3})*[\"!?.:;\\s]*/g This expression splits sentences at ! ? . : ; but not at an ellipsis ...\n  Sentences are split into tokens that are either phrases and words, or spaces and punctuation.\nCorresponding regular expression matches 1) all phrases found on the list, 2) any words, 3) non-words (ie. spaces and punctuation).\nHere is an example with two phrases:\n/of course|get up|(?\u003cwords\u003e[\\p{L}\\p{M}\\'-]+)|(?\u003cnowords\u003e[^\\\\p{L}\\\\p{M}\\'-]+)/gui The Unicode character class \\p is utilized with categories Letter {L} and Mark {M} to account for letters beyond standard ASCII characters.\n  To satisfy the bonus criterion of finding phrases with any punctuation in them, phrases at the beginning need to be replaces with:\n/of[^\\\\p{Letter}\\\\p{Mark}\\'-]+course|get[^\\\\p{Letter}\\\\p{Mark}\\'-]+up/ to allow punctuation or spaces between words that make up the phrase.\nAdditionally, in preparation for parsing all phrases from the users vocabulary, punctuation is stripped off of words.\n  If a token is a phrase (simply checking for spaces suffices), it will be handed over to a Phrase component. Words go into the Word components, and punctuation and spaces are wrapped into \u003cspan\u003es. That is necessary for selecting phrases from the text (see next section).\n  In a Phrase component, a phrase is wrapped into \u003cspan\u003es, and the parsing process from steps 3 and 5 is repeated without phrases.\n  Finally, the Word component wraps \u003cspan\u003es around a word.\n  The CSS for highlighting the words is applied in steps 6 and 7.\n","description":"","tags":null,"title":"Text Parsing","uri":"/challenges/text-parsing/"},{"content":"MVP Confronted with the fully-fledged feature set of commercial language learning applications, our team made the early decision to create a minimum viable product that reflects the essence of what we find crucial in such a product.\n User experience: mobile-first, fast and seamless processing of user interactions Backend and API: layered, robust, RESTful, maintainable, and flexible enough to service front-end needs Database: normalized and withstands scale CI/CD: establish continuous deployment pipelines  Moving Forward Our core team has plans to continue support the maintenance and development of Alexandria.\nEnhance User Experience  Full user access and ownership control:  Add full suite of user access tools, including user password reset. Offer users full control, where words and translations can be deleted as well as updated. User are currently only able to access texts and translations based on their own inputs. Going forward, users will have the choice of accessing a global pool of shared texts and translations, further reducing usage friction, take advantage of other users' contribution, and making the experience more dynamic.   Import/export:  Import / export functionality for interoperability with other learning tools. Import / export functionality for additional file formats, e.g. ebooks formats, subtitle files   Additional language support: Alexandria currently supports 10 languages. We would like to expand the number of languages supported after additional testing. Admin dashboard: Allows admins to make changes to user accounts from the front-end. Mobile optimization: Additional development to further enhance user mobile experiences.  Backend  API enhancements.  Testing  Alexandria back end currently has close to full coverage testing in place. Front end has some unit testing and Cypress testing in place. We plan on introducing testing suite that offers full coverage testing for the front end.  ","description":"","tags":null,"title":"MVP and Future Plans","uri":"/future/"},{"content":"Alexandria’s ability to allow users select and save phrases along with its translation is one of the key features that sets her apart from other open-source or commercial products on the market. This particular functionality was challenging to implement due to the way texts are displayed.\nEach word in a user text resides within its own span. When user clicks on a word, it is set as the currentWord state, triggering a re-render of the translation input component, which contains the current word, existing translations, links to dictionaries and translations, as well as the option for user to set their level of familiarity with the word.\nPhrases on the other hand, are components separate from words. They are also surrounded by highlighted spans based on a user’s level of familiarity with the phrase. We needed a way to create phrases on the fly when a range of words was selected. Such range ought to be added to the userWords state, which would then automatically trigger the highlighting of the new phrase across the text.\nFirst we had to settle on how to select a range of words, and debated two options:\n Selecting the text itself. This would bring the challenge of handling incomplete word selections. Consider, for example, the phrase “better the devil you know”. If a user started and/or ended their selection in the middle of a word, the currentWord might end up being displayed as “er the devil you kn” instead of the intended phrase. It was very important to us that this kind of “sloppy” selection was possible to enhance the user experience instead of forcing users to carefully point the cursor (or worse: their finger) at the exact start and end of the words. Select the span elements containing the words. From a UI perspective we were drawn towards this approach because the span elements are the smallest units of our text, and we would not have the problem of incomplete words.  But while we got a prototype of element selection working on desktop browsers, the problems on mobile browsers were unsurmountable within our given time frame. We turned towards developing a solution using text selection instead.\nUsing the browser API Selection, we were able to leverage anchorNode and focusNode to access where the selection started and ended. The next step was to determine whether the selection was forward (left to right) or backward (right to left), because this would influence how the selection had to be extended to complete words. This was done by spanning a new Range between anchorNode and focusNode and checking whether it was collapsed (meaning the selection was backwards). The selection would be extended in the appropriate directions using the setBaseAndExtend method.\nFinally, the textContent of the Nodes within the selection was extracted and set as the new phrase to be added to the UserWords state.\n","description":"","tags":null,"title":"Phrase selection UI","uri":"/challenges/phrase-selection/"},{"content":" Stephen Krishan (2021): Explorations in Language Acquisition and Use Software Development Community (2019): What Is Service-Oriented Architecture? Yaroslav Lapin (2021) : Jotai vs. Recoil: What are the differences? Edgar F. Codd (1972): Further Normalization of the Data Base Relational Model Roy Fielding (2000): Architectural Styles and the Design of Network-based Software Architectures  ","description":"","tags":null,"title":"References","uri":"/references/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"}]