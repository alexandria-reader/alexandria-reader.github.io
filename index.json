[{"content":"Current “learning with text” software applications fall under two broad groups.\nOne group includes long-standing open source projects that are well received but sparsely maintained, or newer free applications that started with a bang, but were quickly forgotten and subsequently neglected.\nThe second group of applications are commercial solutions that address the goal of language learning with text, but suffer from feature bloat, lack of user friendliness, or mobile performance issues.\nAlexandria would like to become the go-to solution for language learning enthusiasts. Through keeping the product free and open source, we aspire to continue developing product features that are crucial to language learners, while maintaining a joyful user experience based on the latest technological offerings.\n    Existing Free/Open Source Software Alexandria Commercial Products     Interactive front-end user experience ❌ ✅ ✅   Open Source ✅ ✅ ❌   Ongoing Development ❌ ✅ ✅   Mobile First ❌ ✅ ✅   Responsiveness to Community Input ❌ ✅ ✅   Phrase Selection ❌ ✅ ❓    Our main objectives at the initial stage are to: create a modern, fast, open source webapp with a mobile first interface. This will allow users to quickly and easily start learning without struggles with the interface, or excessive wait times for content to load.\n","description":"","tags":null,"title":"Competitor Analysis","uri":"/competitors/"},{"content":"Technical Decisions Every project starts with a couple of choices and ours is no different. From language, to database to front end we had to choose the technologies we felt were right for the scope and demands of our application.\n","description":"","tags":null,"title":"Technical Decisions","uri":"/decisions/"},{"content":"Before we started developing Alexandria, none of us had any real experience using TypeScript, whereas we all had a solid foundation in JavaScript. Yet after doing a lot of research, we decided to use TypeScript in our project. Why?\nStatic typing One of the main reasons is the static typing that is a core part of TypeScript. In JavaScript, an object can have whatever properties you want, and you can add and remove them as you see fit. In TypeScript, types are static, meaning that once an object’s type is declared, it does not change its type and can only take certain values.\nAn example of this is a UserWord which is how we chose to represent the words that a user saved to the database by adding a translation and status.\ntype UserWord = { id?: number, word: string, status?: Status, translations: Array\u003cTranslation\u003e, languageId?: string }; The type UserWord has 5 properties with predefined types. So if, for example, we tried to add a languageId that was a number, we would immediately be warned by the compiler that the wrong type had been added. An even better example is the translations property, which takes an array of Translation type objects. If we tried to add something to the array that was not of type Translation, we would again be warned by the compiler.\nAnother useful feature is that if you try to add a property that has not been declared on the type, for example language, the compiler would warn you that the property language does not exist on type UserWord.\nThe warnings given by the compiler and even by our IDE of choice, VSCode, were invaluable in alerting us to type-related bugs before the code was even run. Over the course of development, this saved us a significant time and frustration as far fewer errors made it into the codebase. Even errors as simple as misspelled variables are caught by the TypeScript compiler.\nCode readability TypeScript greatly increases the readability of our code. Let’s take a function that takes an object as a parameter. In JavaScript, you would have to be familiar with that object and its properties, or possibly log the object to the console to see what properties exist on the object. With TypeScript in VSCode, you can simply over the mouse over the type declaration to see a list of the properties that object will have as well as the types of each of those properties.\nThis means that in future, if more developers join our project, it will be much easier for them to get up to speed on with our codebase. This also results in increased code stability, as you can be sure exactly what properties an object will have, or whether a certain variable is possibly null.\nEasy refactoring TypeScript makes refactoring much easier. For example, if you change a function’s name, but forget to change the name in another file, the TypeScript compiler will immediately alert you to the issue, even showing you the exact file and line of code where the problem resides. This means that you can refactor with confidence, resulting in a cleaner codebase.\nFuture advantages All of this means that as the codebase grows and more developers join the project, it will be easier for them to get up to speed, and the codebase will be more stable.\nDisadvantages of TypeScript Of course, nothing is perfect. Some of the disadvantages of TypeScript include:\nLonger code. Using TypeScript involves writing more code than with JavaScript, meaning that writing the code can take longer than with just plain JavaScript.\nHaving to learn TypeScript. As I mentioned above, none of us had any real experience with TypeScript before beginning this project, so learning TypeScript on top of designing and coding the app resulted in longer development time, especially in the beginning stages. Thankfully, having a strong base in JavaScript made this process much easier.\nTypeScripts static typing is not true static typing. Once TypeScript is compiled, it is transpiled into untyped Javascript, meaning that there is still a small risk of type problems at runtime.\nConclusion Overall, we felt that the benefits clearly outweighed the disadvantages for our use case. It was worth putting in the extra effort of using TypeScript as it helped us catch bugs before the code was run, meaning that they didn’t make it into production. The added readability and easy refactoring made TypeScript the clear winner.\n","description":"","tags":null,"title":"Choosing TypeScript","uri":"/decisions/typescript/"},{"content":"Alexandria is primarily deployed as a web application, where users log in and interact with the information they have stored on the platform. Therefore, robust and scalable database choice is a must.\nOur research comparing between SQL and NoSQL databases centered around the following issues.\n   Parameter SQL NoSQL     Ability to handle high volume of complex queries ✅ ❌   Scalability Vertical Horizontal   Data type Structured Unstructured    SQL Structured Query Language (SQL) is the standard language for dealing with relational databases that define relationships in the form of tables. SQL databases works with a predefined schema, and can be used to effectively insert, search, update, and delete database records. SQL databases are table-based, and known for its vertical scalability.\nNoSQL NoSQL represents non-relational database management systems, where fixed schema is not required. It also avoids joins, and is easy to scale. NoSQL databases can be document based, key-value pairs, or graph databases. It uses dynamic schema for unstructured data, and is known for its ability to scale horizontally.\nChoosing SQL Our choice of SQL is based primarily on its ability to handle a high volume of complex queries. Each user interacting with the Alexandria may upload text, select words from the text to add translations to, change status of a word, or even switch language, all within the same session. Each action requires sometimes complex queries to complete.\nWhile SQL became Alexandria’s database of choice, we are acutely aware of the shortcomings of a SQL database, namely its vertical, versus horizontal scalability. Vertical scalability means that while we can increase the load on a single server by increasing RAM, CPU, or SSD, NoSQL databases handle more traffic by sharding and adding more servers, allowing it to become larger and preferable in the cases for large and ever-changing data sets.\n","description":"","tags":null,"title":"SQL or NoSQL?","uri":"/decisions/sql/"},{"content":"When we first start designing and implementing our backend, we started out with just two layers, routes and services. With this approach, we realized that we were ending up with bloated services as both the data access logic and the application logic resided in the service. Once we saw that this was becoming a problem, we did some research into the best way to structure the backend.\nWe ended up deciding on a three layer design, involving routes, a services layer and a data access layer. This allowed us to separate concerns and create more reusable components.\nWith this layout, when a request is made to a route, it calls the corresponding service, which in turn calls the data access layer. The data access layer simply queries the database and returns the result to the service layer. This means that we could put whatever logic was necessary in the service layer to parse the data, and then pass that data back to the route. The route then can be kept dumb and simply passes the data it receives from the service back to the client.\nThe advantage of this architecture is that the application logic can be kept out of the routes and data access layer. Unit testing is made easier as tests can be designed to test each part individually. This also has the added advantage that if we ever decided to change our database implementation, most of the changes would be confined to the data access layer.\n","description":"","tags":null,"title":"3 Layers for the Back End","uri":"/decisions/backend/"},{"content":"Project requirements In order to achieve both performance and user experience objectives for Alexandria, we settled on a number of goals that we’d like to fullfil at the initial stage.\nOne of the key features of the app is to reflect the changes made by a particular user interaction immediately, without an additional page reload.\nConsider the following user actions and their effects on a piece of text. Our challenge is to make every user interaction feel instantaneous and seamless.\n   User action Immediate Rendering     Click on word/phrase Translation input box available   Click on highlighted word/phrase Translation and status displayed   Update word/phrase status Highlighted colour on word changes    Another challenge is the ability of the UI to keep in sync with state changes of variables. With increased complexity, it becomes harder to track the exact state of the UI at every given moment during the lifecycle of the application. The challenge is to represent the UI accurately after a possible flurry of user interactions.\nPlain Javascript versus React Pitting plain JS against React against Alexandria’s initial requirements, React was the easy winner. Below is a summary of some of the differences between the two languages as it pertains to Alexandria.\nRendering efficiency With Javascript, UI is created in HTML on the server side, and sent to the browser. Each user interaction resulting in a data change requires an API call to the backend, and a re-rendering of the browser based on the new DOM. We briefly considered templating engines like Handlebars. While they are excellent choices for server-side rendering, they do not meet Alexandria’s goal of delivering speedy incremental updates of the DOM, event-handling, or frontend to backend communication.\nOn the other hand, React defines UI on the browser. An app starts with a blank container and loads the UI. The UI is defined by a component that returns the JSX - a HTML lookalike. The new component is rendered into the div container using the ReactDOM library, and the result will appear directly on the browser.\nBecause each UI is broken into smaller components, each component can render independently due to its use of the virtual DOM. One change in a component does not result in the re-rendering of the entire page, merely the changed component, thus improving the front-end user experience significantly.\nState management In JS, an event listener is attached to a DOM element to listen for changes. With each state change, we track the value deviation function, re-run the deviation functions and track the changes in return value. Those changed values are passed to the appropriate DOM element through the DOM API.\nWith React, the UI is set up to keep the entire state of a variable in the form of a “controlled component”, while an event (e.g. a button press) can be specified directly in the code, with no need for an event listener. The change to variable is registered, and the UI is updated automatically. There’s no need to go into the DOM to find the variable to update.\nBeing able to design components that hold state was fundamental to the design of Alexandria. Using React, each “word” residing within a piece of text can have it’s own state (translation, learning stage, context). When a user changes a word’s state, the result is displayed immediately on the DOM. In this case, the word would be highlighted, and its translation and context would be saved to the word’s state.\nReact’s implementation of hooks and component state fit perfectly with this requirement.\nMaintainability and Reusability In JS, the markup and functionality are kept separate. As apps grow bigger, this becomes a major source of complexity, where a developer needs to keep track of both sets of codes.\nIn React, the app is split into components, where each component maintains the code required to both display and updates the UI. This also allows reusability of components, another benefit.\nRecoil When it comes to choosing a state management tool, we began by assessing what the application needed.\nIt was immediately clear to us that users will be interacting intensively with highly dynamic components: highlighted words, translations, status, and languages are all expected to change with every user interaction. Therefore, it was vital that after each update, each component state is accurately captured and rendered back to the users in the most efficient manner.\nTurning our attention to state management tools available on the market, we quickly realized although the number of libraries and packages are numerous, most belong to one of three categories in addition to the React natively supported Context API: Flux (Redux, Zustand), Proxy (Mobx, Valtio), or Atomic (Recoil, Jotai) [2].\nWithin the context of Alexandria, we took a closer look at three choices: React’s Context API, Redux, and Recoil. Redux has long been the state management library of choice for applications of a certain size, while React’s very own Context API provided state management directly from React itself. Adding to the mix, new kid on the block Recoil had been released by Facebook, and touted as a React-specific state management library maximizes developer happiness.\nChoosing the right tool for the job took some careful examination. While we did not perform an exhaustive benchmarking study, some initial sandboxing helped us arrive at the following observations.\nContext API How it works A context provider is created on the parent component where the global state sits. Data is passed down the tree to its children components. Those children components are able to access the context object from the parent through props drilling.\nThis works great when the context data changes only occasionally. But the solution breaks down when we store substantive amount of data in the context. In the case of Alexandria, the context data would change every time the user interacts with one word or phrase in the text, and causes unnecessary re-rendering of all components of the provider, in this case, all of the words/phrases.\nAlternatively, the context object can be broken down into multiple states. This, however, introduces complexity into the codebase that could be avoided through another approach.\nPros  Context API is part of React’s built-in package, requiring no additional installation Relatively easy to set up and start working Great for static or rarely refreshed data  Cons:  Excessive unnecessary re-rendering affects performance  Redux How it works Instead of changing state by propagating values through the component tree, Redux takes a different approach. When a component state changes, the component dispatch an action to a reducer. The reducer handles the action (e.g. change a word state), manipulates the old state to reflect the new state. The reducer goes to the central store, which that manages all the states of the application. The application subscribes to part or all of the store, which then passes updated states as props to components that subscribe to it. This is how the application handles a state change.\nPros  Great for often-refreshed data, only listen to what has changed, only the value changed re-renders Integrates with react-redux library Server-side rendering possible  Cons  Much more complicated to get started, introduces substantial complexity to application  Recoil How it works Recoil was created to solve a number of challenges very specific to large and interactive apps, where issues of persistence and robustness are central. More specifically, how does one keep different branches of the tree in sync where components live in different branches of the tree? How does one avoid re-rendering multiple components when state change(s) are detected in the same provider?\nIn order to meet these global state management challenges, Recoil implemented small, shared units of state named “atoms”, and derived states named “selectors”. Atoms are changeable and subscribable pieces of state. That is to say, one particular atom can be subscribed to by components, and atoms can be used to derive data from state. States are stored within the React tree, similar to how useState and useContext hooks function, making it easy to get started. Recoil encourages separation of state into separate pieces in order to route components. The existence of atoms is particularly well-suited for applications that receive a high frequency of updates.\nPros  Easy to set up and implement, experience of working with useRecoilState similar to working with useState Re-renders only components with changed state, improves performance  Cons  Library still in beta  Choosing Recoil Recoil became our state management library of choice based on two key considerations. One, it delivered optimal performance by rendering only component that changes upon user interaction. Two, it offered maximal developer happiness. The simplicity of set up and usage made state management a non-issue, where state changes are central to the viability of Alexandria.\n","description":"","tags":null,"title":"React and Recoil","uri":"/decisions/react-recoil/"},{"content":"Database and API With the technical decisions settled, it was time to design two vital aspects of the backend architecture: the database and the API.\nWhile a database of some form would always have been necessary, exposing an API was the result of going with a single page React app that talks to the server via API calls. This also opens the door for alternative front ends at a later stage - a mobile app could use the same back end. It is also in line with Alexandria being an open source project, allowing for decoupled contributions to front end or back end.\n","description":"","tags":null,"title":"Database and API","uri":"/database-api/"},{"content":"Normalization To normalize a database means to organize or structure it in a way that avoids redundancy of data, undesirable dependencies, and inconsistencies.\nIn his paper “Further Normalization of the Data Base Relational Model\" [3], Edgar F. Codd, the “father” of the relational database model, listed these objectives of normalization:\n To free the collection of relations from undesirable insertion, update and deletion dependencies. To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs. To make the relational model more informative to users. To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.  The following set of tables serves as an example for a database structure that is not properly normalized.\nTable 1:    username word word_language translation translation_language     Mark house en Haus de   Dana house en maison fr   Eamon casa es house en    Table 2:    id username email is_admin     1001 Dana dana@email.com true   1002 Eamon eamon@email.com true   1003 Mark marc@email.com false    The first table holds values of four key entities of Alexandria: users, words, translations, and languages. The second table is the users table to go with it.\nThere are two independent username columns, an example of redundancy that can lead to so-called “update anomalies”: It turns out that Marc is not spelt with a “k” but with a “c”. If we only change this in the users table, the data will become inconsistent, because it still says “Mark” in the first table. The username needs to be updated in two different places which is prone to errors.\nNormal forms Normalization of relational databases is guided by rules that build on each other. They are called “normal forms”.\nTo satisfy the the first normal form (1NF), data sets must not contain tables themselves, and each data set must have a unique primary key. In the example above, there are no nested tables, but only the users table has a primary key (id), so not even 1NF is satisfied.\nThe second normal form (2NF) fulfills the rules of the first normal form, and it also demands every attribute of a dataset is functionally dependent on the primary keys. Those attributes that do not depend on the primary key should go in their own table and be linked with a foreign key instead.\nThe third normal form (3NF) adds the rule that transitive functional dependencies must be eliminated as well: If B depends on primary key A, but C depends on B, then both B and C should be extracted to their own table.\nThere are higher normal forms but they are rarely encountered in the wild. If our database were to satisfy 3NF it would be safe from insertion, update and deletion anomalies. Did we succeed with our design? Let’s find out…\nTables. More tables! No, sorry, fewer tables again. We started out with a couple of fairly basic design. This is the graphical representation of one the first drafts:\nBut as we let our imagination run wild the demand for tables and table columns grew in line with our feature list. “Hey, wouldn’t it be nice if the user could choose from other users' translations?”, or “I think every time an existing translation is applied, it should save a new context.”, or “Will users be able to set a preferred web dictionary?”.\nAfter a few iterations to allow for more features, especially cross-user content we landed here:\nReality eventually got the better of us and we decided to cut down the feature list to a more manageable size in line with our time table. We focussed on the single user experience with only the basic groundwork laid for cross-user content, simplified the user profile, and significantly simplified context handling.\nThis is what we arrived at for the initial release of Alexandria:\nAs you can see, for the final design we stripped the connecting tables of their own unique ids, and made composite keys - the combinations of the foreign key columns - the primary keys. This would ensure that the connections are unique and a user cannot have, for example, two statuses for the same word.\nAdmittedly it should also have been done for the words table, because each word (ie. combination of letters) is unique in its language. But this occurred to us too late into the project and we stuck to the id as primary key for words.\nIt is for this small detail that our database does probably not full satisfy 3NF, but it is close enough to achieve all the objectives of normalization.\nNaming things A useful side effect of designing the database very early in the project was that we had to agree on the names of the entities that we were going to deal with in our code. Eg. what is a word, what is a phrase? (They are actually the same in the database but different for text parsing). Finding that common language was essential for all communication going forward.\nAlexandria’s MVP database tables Main tables    table description     users The people using Alexandria. Each user can actively learn one language at a time and translate to one base language. If the language preferences change, previous progress (translations, word statuses) is kept.   admins The people administering Alexandria. That would be us for now.   languages The languages that are supported by Alexandria. For the first release, it is a list of ten. What they have in common is their use of a latin-based alphabet (to avoid surprises with our text parser for now) and their being supported by the PostgreSQL text search. A flag representing the language is saved as a unicode character combination to be used in menus across the application.   texts The key entity for learning. Texts are currently provided by users through forms, later through URLs and files. Texts default to being private which should particularly suit those who learn best by reading state secrets or erotic literature. Public texts, that can be used by every user to study, could be an option for a future release. The database already provides the necessary column.   words Translatable entities. Usually single words like cabbage, but also compounds, or phrases, like get up or l’hôtel. Each word must be unique within its language. It can have different meanings (translations) but in database terms, each combination of letters must occur only once per language.   translations Translations of words can be provided or selected by the users, but they exist independently of the users who created them. The go from the language of the word in question to a target language associated with this specific translation. Translations could also also be automatically generated to seed the database with a language’s most common words; another feature for a follow up version.   webdictionaries Online resources in which the user can look up a word or phrase. They    Connecting tables    table purpose     users_words Word statuses for a usr-word combination are “learning”, familiar”, and “learned”.\nUser dictionaries could be generated using this table.   users_translations Every translated word originates in a text, and the surrounding words from that text form the translation’s context. Users have their own contexts for every translation. If the translation is not newly provided but simply selected from previous translations, the current context is saved and connected to selected the translation and user.   webdictionary_preference Users can have one favourite dictionary per source language.    ","description":"","tags":null,"title":"Database Architecture","uri":"/database-api/database/"},{"content":"In the design of the API that is queried by the React front end, we aimed at following REST principles as outlined in Roy Fielding’s 2000 dissertation [3], but within the constraints of our type of application and of the current state of development.\n Client-Server: As mentioned previously, this separation came naturally with the decision to create a React-based single page application. Stateless: For security reasons and device interchangeability, only the user id is stored with the client in a JSON Web token which is sent with every request. Additional information is always drawn from the user state which is kept in a Recoil atom. The request then includes the combined state information and sends it to the server. Cacheable: The core of Alexandria, the single text and translation page is a highly dynamic affair where cached responses for word and translation queries could seriously damage the user experience. We decided to only explicitly allow caching of responses for resources that do not change often: the list of languages, the web dictionary list, and the individual texts. Layered System: Front end and back end of Alexandria reside on different servers, allowing for independent scalability. Code-on-Demand: This was not actively implemented in Alexandria Uniform Interface: According to Fielding, “In order to obtain a uniform interface, multiple architectural constraints are needed to guide the behavior of components: …  identification of resources, manipulation of resources through representations, self-descriptive messages, hypermedia as the engine of application state.”    During the API design we spend most of our time on getting the uniform interface right, or, more prosaically, come up with a system of URLs (routes) that - in combination with request bodies - would allow the client to get any response it requires from the server, and be logical and readable at the same time, to the API as accessible to new contributors as possible. Here is what we ended up with:\nResources and collections  Resources are the individual items (rows) of the main database tables (the once with a “single noun” name). Each resource has a unique id within its table. Collections are lists of resources, eg. words or texts, that can be filtered by criteria such as languages or user.  Route patterns Basic access  The type of the resource that is queried is always the first part of the route. This is the case for both individual resources and collections. The queried resource is named in plural form. Individual resources are accessed by their id. Data from other resources associated with the queried resource might be sent back as well, but querying those happens on the backend and is not exposed via the API. Creation, updates and deletion of resources happens via the basic route, ie. without filters (see below).  Examples  GET /texts/ =\u003e a collection of all texts GET /texts/38422 =\u003e the text with id 38422 POST /translations =\u003e create a new translation PUT /users =\u003e update user data DELETE /texts/38422=\u003e removes text with id 22533  Filtering collections  If the second part of the route is not an id, we are looking for a filtered collection of that resource. Filters are indicated by a keyword (in singular) and a unique identifier. Filters can be chained. Filtering for the logged-in user is indicated via the URL. Filtering collections down to to those resources belonging to a user is done on the back end server based purely based on the functionality and access rights of the app.  Examples  GET /words/text/17/language/fr =\u003e get all words from text 17 that have a translation to French (unique identifier fr) GET /texts/language/fr =\u003e all texts in French (unique identifier de) belonging to the current user  Special routes Certain functionality that the client demands from the server did not fit the resource schema outlined above, for example login or logout, or certain user related functions. In those cases, the route URLs deviate from the pattern after the initial resource and simply spell out what is expected of them.\n","description":"","tags":null,"title":"API Design","uri":"/database-api/api/"},{"content":"Challenges and Solutions While the designing database and API were challenging in and by themselves, and the thorough preparation definitely paid of during the coding stage, there were, of course, still hurdles to overcome, problems to solve, solutions to find.\n","description":"","tags":null,"title":"Challenges and Solutions","uri":"/challenges/"},{"content":"One challenge we faced early on when implementing the back end and database was how to automate testing of pull requests.\nIn the beginning, while we had implemented some basic unit testing, when reviewing a PR we often had to manually test routes using Postman to ensure everything was working correctly before merging. Obviously this was not sustainable, especially as the project grew in size and scope.\nWe were able to achieve a good level of coverage using Jest quite quickly, but one problem we ran into was that each user was using their own local testing database, which sometimes lead to different outcomes. To improve the reliability of the tests, we wanted to make sure that the database was always in a known state when the tests were run, allowing for greater control.\nTo do this, we set up a PostgreSQL Docker image that would automatically spin up a Docker instance, set up and seed the database each time the tests were run. One of the inherent advantages of using a Docker image is that unless explicitly set up, data is not persisted after the Docker instance is shut down. We were able to take advantage of that to avoid having to reset the database before running tests. This made the testing process much more reliable and easier to control.\nInitially, a reviewer had to download the PR branch, then run the test suite manually to ensure there had been no regression. We decided to implement automated testing to speed up development and make the reviewers job easier.\nAfter some investigation, we decided to use GitHub actions to automate the tests. We set the automated tests on GitHub to use the same PostgreSQL Docker image and seed data as we were using on our local machines. As we were already using GitHub, this greatly simplified the review process as the test results are very prominently displayed on the PR, instantly notifying the developer if there was a problem that needed to be fixed.\nThis solution ended up greatly simplifying the review process, and saved us a lot of time as bugs and regression were caught quickly.\n","description":"","tags":null,"title":"Automating Tests","uri":"/challenges/automated-tests/"},{"content":"Finding the words and phrases in the text We have already discussed why we chose to work with a relational database as opposed to a document-based one. But our decision to use PostgreSQL brought another benefit with it, and that is the build in full text search which we put to good use to solve.\nOn the front end, Alexandria highlights all the words and phrases that the user has already encountered. To find out whether a text contains words or phrases that the user has already marked, the words of the text have to be compared to all the user’s words and phrases saved in the database.\nWe were already working on a search algorithm using tries, when we had a closer look at the PostgreSQL documentation and decided that its full text search was just what we needed, and that using it would bring a couple of advantages with it:\n It is written in C and most likely faster than anything we would implement in TypeScript. It is happening at the starting point of the data chain, so no unnecessary data has to be transferred and processed by neither the server nor the client.  PostgreSQL text search works by parsing and normalizing a text into a text search vector of the tsvector data type. Search terms on the other hand are parsed and normalized into text search queries of the tsquery type. The parser breaks a string (text or search term) into individual tokens, mostly words in our case.\nThe normalization then removes common small words from the tokens and reduces the words to their stems according to language specific dictionaries. Heroku’s PostgreSQL installation came with support for twenty languages.\nWhile this normalization step is very powerful, it is obviously geared for full text search, allowing for results like “handling” and “handlebar” when search term is “handle”. But this was a bit too lenient for our purpose. For the language learner “handle”, and “handling” should be considered as two words. Luckily, PostgreSQL also ships with a simple configuration that simple parses the text but leaves the tokens as they are, and that is the configuration we used.\nTo save processing time, each text is parsed immediately into a tsvector when it is added to the database (or modified later). The vector is saved in a column of the texts table defined like this:\nALTER TABLE texts ADD COLUMN tsvector_simple tsvector GENERATED ALWAYS AS (to_tsvector('simple', title || ' ' || body)) STORED In a similar fashion, every word or phrase added to the database also gets a column in which the parsed tsquery is saved.\nWe then find all the words in text 5 that user 1 has in their vocabulary with this SQL query:\nSELECT w.id, w.word, uw.word_status FROM words AS w JOIN users_words AS uw ON w.id = uw.word_id WHERE uw.user_id = 1 AND w.language_id = (SELECT t.language_id FROM texts AS t WHERE t.id = 5) AND w.tsquery_simple @@ (SELECT t.tsvector_simple FROM texts AS t WHERE t.id = 5); Parsing the text into React and HTML Eventually, thanks to the powers of PostgreSQL, a text and a list of words from the user’s vocabulary will make their way to the client. The next step would then be combining these two in a user interface that satisfied these conditions:\n Every occurrence of a word from the list must be highlighted in the text according to its level of familiarity. Every occurrence of a phrases must be highlighted in the text according to its level of familiarity. Each word from the text must be, regardless of whether a word is highlighted, or whether it is part of a phrase. Each phrase must be clickable/selectable. Punctuation and spaces must be part of any words (with the exception of the apostrophe and hyphen). The text must be split into sentences to facilitate saving the context of a word when adding a new translation. Bonus: Phrases should be highlighted regardless of the punctuation with which is was saved previously or which is encountered in the text.  It was clear from the beginning that on the lowest level it meant that words would end up nested in \u003cspan\u003es, and that a phrase meant another set of \u003cspan\u003es around those word \u003cspan\u003es . It was also clear that simply splitting the text at the whitespace characters would not do the trick because commas or exclamation marks would stick to their preceding word. As always in such cases, it was regular expressions to the rescue!\nFrom the full text body to the individual words the text is parsed and split up into React components and HTML elements in these steps.\n  The text body is divided into Paragraph components, using the split string method.\n  Paragraphs are split into Sentence components using this regular expression:\n/[^\\s]([^!?.:;]|\\.{3})*[\"!?.:;\\s]*/g This expression splits sentences at ! ? . : ; but not at an ellipsis ...\n  At the sentence level it gets interesting. Sentences are split into tokens which can either be phrases, words, or spaces and punctuation.\nThe corresponding regular expression matches all phrases as found on the list or alternatively any words or non-words (ie. spaces and punctuation). Here is an example with two phrases:\n/of course|get up|(?\u003cwords\u003e[\\p{L}\\p{M}\\'-]+)|(?\u003cnowords\u003e[^\\\\p{L}\\\\p{M}\\'-]+)/gui It utilizes the unicode character class \\p with the categories Letter {L} and Mark {M} to account for letters beyond the standard ASCII characters.\n  To satisfy the bonus criterion of finding phrases with any punctuation in them, the phrases at the beginning need to be replaces with\n/of[^\\\\p{Letter}\\\\p{Mark}\\'-]+course|get[^\\\\p{Letter}\\\\p{Mark}\\'-]+up/ to allow punctuation or spaces between the words that make up the phrase.\nAdditionally, as preparation for the parsing all phrases from the users vocabulary for the are stripped off their punctuation.\n  If a token is a phrase (simply checking for spaces suffices), it will be handed over to a Phrase component, Words go into Word components, and punctuation and spaces are unceremoniously wrapped into \u003cspan\u003es. That is necessary for selecting phrases from the text (see next section).\n  In a Phrase component, the phrase wrapped into \u003cspan\u003es, and the parsing from steps 3 and 5 is repeated, just without phrases.\n  Finally, the Word component wraps \u003cspan\u003es around a word.\n  The CSS for highlighting the words is applied in steps 6 and 7.\n","description":"","tags":null,"title":"Text Parsing","uri":"/challenges/text-parsing/"},{"content":"MVP Confronted with the fully-fledged feature set of commercial language learning applications, our team made the early decision to create a minimum viable product that reflects the essence of what we find crucial in such a product.\n User experience: mobile-first, fast and seamless processing of user interactions Backend and API: layered, robust, RESTful, maintainable, and flexible enough to service front-end needs Database: normalized and withstands scale CI/CD: establish continuous deployment pipelines  Moving Forward Our core team has plans to continue support the maintenance and development of Alexandria.\nBackend  API enhancements.  Frontend   Full user access and ownership control:\n Add full suite of user access tools, including email validation upon sign-up and password reset. Offer users full control, where words and translations can be deleted as well as updated. User are currently only able to access texts and translations based on their own inputs. Going forward, users will have the choice of accessing a global pool of shared texts and translations, further reducing usage friction, take advantage of other users' contribution, and making the experience more dynamic.    Starter kit upon sign-up: Offer new users a package of existing texts to improve onboarding experience\n  Import/export: Import / export functionality for interoperability with other learning tools\n  Additional language support: Alexandria currently supports 10 languages. We would like to expand the number of languages supported after additional testing.\n  Admin dashboard: Allows admins to make changes to user accounts from the front-end.\n  Mobile optimization: Additional development to further enhance user mobile experiences.\n  Testing  Alexandria backend currently has close to full coverage testing in place. Front end has some unit testing and Cypress testing in place. We plan on introducing testing suite that offers full coverage testing for the front end.  ","description":"","tags":null,"title":"MVP and Future Plans","uri":"/future/"},{"content":"One of the most important features of our app is allowing users to select and save phrases along with its translation. This ended up being more challenging than expected due to the way our texts are displayed. Each word in a user text has its own state, and resides in its own span. When a user clicks on a word, it is added to the currentWord state, which then triggers a re-render of the translation input component, which contains the current word, existing translations, links to dictionaries and translations, as well as the option for the user to set their level of familiarity with the word.\nPhrases are their own component, separate from words. They are also surrounded by spans which are highlighted based on the users level of familiarity with the phrase. So we needed a way to create phrases on the fly as users selected them. So when a user selects several words, their selection is added to the userWords object, which automatically highlights the new phrase.\nThis sounds relatively simple, but the challenges appeared depending on how the user selected the word. Consider, for example, the phrase “for example”. If the user started or ended their selection in the middle of a word, the currentWord might end up being displayed as “or examp” instead of “for example”.\nUsing the browser API Selection, we were able to get the anchorNode and focusNode, the nodes where the selection started and ended. With access to those nodes, we could extract the full words using the nodes textContent property, meaning it was trivial to replace the first and last words of the selection with the full words. This gives the user a much better experience as they can select phrases without having to be careful to always select full words.\nThe only problem with this approach was that if for some reason a user selected a phrase dragging the mouse from right to left, the first and last word would be reversed. We tried to solve this by saving the X location of the mouse pointer onMouseDown and comparing it to the end X location onMouseUp to see if the selection was done backwards or not. But when a selection was done across multiple lines and the start point was further left than the end point, it didn’t work.\nAs we had access to the context (the phrase the selection was from), in the end we were able to simply test if the phrase appeared in the context, and if not, switch the first and last words, thus greatly simplifying the selection logic.\n","description":"","tags":null,"title":"Phrase selection UI","uri":"/challenges/phrase-selection/"},{"content":" Stephen Krishan (2021): Explorations in Language Acquisition and Use Yaroslav Lapin (2021) : Jotai vs. Recoil: What are the differences? Edgar F. Codd (1972): Further Normalization of the Data Base Relational Model Roy Fielding (2000): Architectural Styles and the Design of Network-based Software Architectures  ","description":"","tags":null,"title":"References","uri":"/references/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"}]